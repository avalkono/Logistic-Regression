---
title: "Logistic Regression HW 3"
output: html_document
date: "2024-09-11"
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(car)
library(stats)
library(UsingR)
library(AmesHousing)
library(car)
library(DescTools)
library(corrplot)
library(mgcv)
library(visreg)
library(Hmisc)
library(ROCit)
library(MASS)
library(survival)
library(pROC)
```

```{r}
train_bin <- read.csv('insurance_t_bin.csv')
validation_bin <- read.csv('insurance_v_bin.csv')

#Replace missing values with missing category
train_bin = train_bin %>% replace(is.na(.), 'M')
validation_bin = validation_bin %>% replace(is.na(.), 'M')

#Fix separation issues by combining categories
train_bin$CASHBK.c <- as.character(train_bin$CASHBK)
train_bin$CASHBK.c[which(train_bin$CASHBK > 0)] <- "1+"
validation_bin$CASHBK.c <- as.character(validation_bin$CASHBK)
validation_bin$CASHBK.c[which(validation_bin$CASHBK > 0)] <- "1+"

train_bin$MMCRED.c <- as.character(train_bin$MMCRED)
train_bin$MMCRED.c[which(train_bin$MMCRED > 2)] <- "3+"
validation_bin$MMCRED.c <- as.character(validation_bin$MMCRED)
validation_bin$MMCRED.c[which(validation_bin$MMCRED > 2)] <- "3+"

#make all variables factors
train_bin <- data.frame(lapply(train_bin, as.factor))
validation_bin <- data.frame(lapply(validation_bin, as.factor))
```

```{r}
final.model <- glm(INS ~ DDA + NSF + IRA + INV + MTG + CC + DDABAL_BIN + CHECKS_BIN + 
    TELLER_BIN + SAVBAL_BIN + ATMAMT_BIN + CDBAL_BIN + ILSBAL_BIN + 
    MMBAL_BIN + DDA:IRA, data=train_bin, family = binomial(link = "logit"))

car:: Anova(final.model,  test = "LR", type = "III", singular.ok = TRUE)
```

```{r}
#Concordance percentage
survival::concordance(final.model)

#Discrimination coefficient
train_bin$p_hat <- predict(final.model, type = "response")

p1 <- train_bin$p_hat[train_bin$INS == 1]
p0 <- train_bin$p_hat[train_bin$INS == 0]
coef_discrim <- mean(p1) - mean(p0)
coef_discrim

ggplot(train_bin, aes(p_hat, fill = factor(INS))) +
  geom_density(alpha = 0.7) +
  scale_fill_manual(values = c("orange", "#56B4E9"), label = c("Non-purchasing", "Purchasing")) +
  labs(x = "Predicted Probability", y= "Density",
       fill = "Outcome", title = "Discrimination of Purchasing and Non-Purchasing Customers") +
      theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = 'bold', size=14), plot.subtitle = element_text(hjust = 0.5, size=13), axis.title = element_text(size=13, face='bold'), axis.text = element_text(color='black', size=10), axis.title.x = element_text(margin = margin(t=10)), axis.title.y = element_text(margin = margin(r=10)), legend.title = element_blank(), legend.position = 'top')

#ggsave(plot1, file = "hist.png", width = 7, height = 4, dpi = 700)
```

ROC Curve
```{r}
#ROC curve
logit_roc <- rocit(train_bin$p_hat, train_bin$INS)
plot(logit_roc)$optimal
summary(logit_roc)

roc_obj <- roc(train_bin$INS, train_bin$p_hat)

roc_data <- data.frame(
  specificity = roc_obj$specificities,
  sensitivity = roc_obj$sensitivities
)

# Plot the ROC curve using ggplot
optimal_cutoff <- data.frame(
  x = 0.3220369,  # 1 - Specificity
  y = 0.7933516,   # Sensitivity
  label = "Optimal (Youden Index) Point"
)

ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "blue", size = 1) +
  geom_point(data = optimal_cutoff, aes(x = x, y = y), color = "blue", size = 5, shape = 3, stroke = 1) +
  geom_point(data = optimal_cutoff, aes(x = x, y = y), color = "blue", size = 2, shape = 19) +
  geom_text(data = optimal_cutoff, aes(x = x, y = y, label = label), vjust = 0.9, hjust = -0.15, color = "blue", size = 3.5) +
  geom_abline(linetype = "dashed", color = "darkgrey") +
  labs(title = "ROC Curve",
       x = "False Positive Rate",
       y = "True Positive Rate") +
  theme_minimal() + theme(plot.title = element_text(hjust = 0.5, face = 'bold', size=16), axis.title = element_text(size=13, face='bold'), axis.text = element_text(color='black', size=10), axis.title.x = element_text(margin = margin(t=10)), axis.title.y = element_text(margin = margin(r=10))) 

#ggsave(plot1, file = "hist.png", width = 7, height = 4, dpi = 700)
```

KS Statistic:
```{r}
ksplot(logit_roc)$`KS stat`
ksplot(logit_roc)$`KS Cutoff`
```

Classification metrics on validation data:
```{r}
#Confusion matrix
validation_bin$p_hat <- predict(final.model, newdata = validation_bin, type = "response")
validation_bin <- validation_bin %>% mutate(INS_hat = ifelse(p_hat > 0.297, 1, 0))
table(validation_bin$INS_hat, validation_bin$INS)

#Accuracy
mean(validation_bin$INS_hat == validation_bin$INS)

#Lift
logit_roc_v <- rocit(validation_bin$p_hat, validation_bin$INS)
logit_lift <- gainstable(logit_roc_v)
print(logit_lift)

#Lift chart
plot(logit_lift, type = 1)

#Lift chart in ggplot
lift_data <- data.frame(
  Lift = logit_lift$Lift,
  Clift = logit_lift$CLift,
  CObs = logit_lift$CObs)

lift_data <- lift_data %>% mutate(Percentobs = CObs/2124)

lift_data_long <- lift_data %>%
  pivot_longer(cols = c(Lift, Clift), names_to = "Metric", values_to = "Value")


ggplot(lift_data_long, aes(x = Percentobs, y = Value, color=Metric, shape=Metric)) +
  geom_line(size=1) +
  geom_point(size=4) +
  labs(title = "Lift Chart", x = "Population Depth", y = "Lift, Cumulative Lift") +
  theme_minimal() +
  geom_hline(yintercept = 1, linetype = "dashed", color = "darkgrey") + 
  scale_x_continuous(labels = scales::percent) + 
  scale_color_manual(values = c("Lift" = "blue", "Clift" = "orange"), labels = c("Cumulative Lift", "Lift")) +   
  scale_shape_manual(values = c("Lift" = 19, "Clift" = 18), labels = c("Cumulative Lift", "Lift")) +
  theme(plot.title = element_text(hjust = 0.5, face = 'bold', size=17), axis.title = element_text(size=13, face='bold'), axis.text = element_text(color='black', size=10), axis.title.x = element_text(margin = margin(t=10)), axis.title.y = element_text(margin = margin(r=10)), legend.text = element_text(size = 12), legend.title = element_blank(), legend.position = 'top')

#ggsave(plot1, file = "lift.png", width = 7, height = 4, dpi = 700)

```
The number of true positives and true negatives exceeded incorrect predictions. The false positive rate was approximately 15% while the false negative rate was about 45%, meaning the model failed to predict nearly half of all purchasing customers but made fewer mistakes when predicting non-purchasing customers. Overall, the model was 70.2% accurate, correctly predicting customer purchases 70.2% of the time. 




```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(car)
library(stats)
library(UsingR)
library(AmesHousing)
library(car)
library(DescTools)
library(corrplot)
library(mgcv)
library(visreg)
library(DataExplorer)
library(Hmisc) 
library(ROCit)
```
## Homework 3
## Fix data
Read in data and replace missing values
```{r}
train <- read.csv("insurance_t_bin.csv")
valid <- read.csv("insurance_v_bin.csv")
#Determine which variables have the highest amount of missing values in train
#sort(colSums(is.na(train)), decreasing = TRUE)
train$HMOWN[is.na(train$HMOWN)] <- "M"
train$INV[is.na(train$INV)] <- "M"
train$CC[is.na(train$CC)] <- "M"
train$CCPURC[is.na(train$CCPURC)] <- "M"
#Determine which variables have the highest amount of missing values in test
#sort(colSums(is.na(valid)), decreasing = TRUE)
valid$HMOWN[is.na(valid$HMOWN)] <- "M"
valid$INV[is.na(valid$INV)] <- "M"
valid$CC[is.na(valid$CC)] <- "M"
valid$CCPURC[is.na(valid$CCPURC)] <- "M"
```
## Fix convergence problems
Fix convergence issues. (I assume you have to fix the validation set as well?)
```{r}
train$CASHBK[train$CASHBK >= 1] <- "1+"
train$MMCRED[train$MMCRED >= 2] <- "2+"
valid$CASHBK[valid$CASHBK >= 1] <- "1+"
valid$MMCRED[valid$MMCRED >= 2] <- "2+"
train[] <- lapply(train, as.factor)
valid[] <- lapply(valid, as.factor)
train$INS <- as.numeric(as.character(train$INS))
valid$INS <- as.numeric(as.character(valid$INS))
```
## Model selected from phase two
Selected the model used from phase two. 14 main effect variables and 1 interaction
```{r}
model <- glm(INS ~ DDA + NSF + IRA + INV + MTG + CC + DDABAL_BIN + CHECKS_BIN + TELLER_BIN + SAVBAL_BIN + ATMAMT_BIN + CDBAL_BIN + ILSBAL_BIN + MMBAL_BIN + DDA:IRA, data = train, family = binomial(link = "logit"))
rank = car::Anova(model)
df = as.data.frame(rank[3])
colnames(df)[1] <- "Pvalues"
df %>% arrange(Pvalues)
```
## Probabilty metrics
### Concordance
```{r}
train$p_hat <- predict(model, type = "response") 
p1 <- train$p_hat[train$INS == 1]
p0 <- train$p_hat[train$INS == 0]
# Concordance
somers2(train$p_hat, train$INS)
# Rank order statistics
coef_discrim <- mean(p1) - mean(p0)
ggplot(train, aes(p_hat, fill = factor(INS))) + geom_density(alpha = 0.7) + scale_fill_grey() + labs(x = "Predicted Probability", fill = "Outcome", subtitle = paste("Coefficient of Discrimination = ", round(coef_discrim, 3), sep = ""))
```
The concordance is 0.7997675 and the coefficient of discrimination is 0.246 
## Classification Metrics
### ROC-Curve and K-S Statistic
```{r}
logit_meas <- measureit(train$p_hat, train$INS, measure = 
                          c("ACC", "SENS", "SPEC"))
#print(logit_meas)
# ROC curve
train$INS <- as.numeric(as.character(train$INS))
logit_roc <- rocit(train$p_hat, train$INS) 
plot(logit_roc)$optimal
#summary(logit_roc)
#K-S statistic
ksplot(logit_roc)
ksplot(logit_roc)$`KS Stat`
ksplot(logit_roc)$`KS Cutoff`
```
The optimal cutoff point is 0.2970672 (K-S statistic)
## Classification metrics using validation data
### Confusion matrix, Accuracy, Lift
```{r}
valid$p_hat <- predict(model, newdata = valid, type = "response")
valid <- valid %>%
  mutate(INS_hat = ifelse(p_hat > 0.2970672, 1, 0))
table = table(valid$INS_hat, valid$INS)
table
Accuracy = (table[1] + table[4])/(table[1] + table[2] + table[3] + table[4])
Accuracy
logit_roc <- rocit(valid$p_hat, valid$INS) 
logit_lift <- gainstable(logit_roc) 
print(logit_lift)
plot(logit_lift)
```
Accuracy on the validation set is 70.20%



```{r}
validation_bin$p_hat <- predict(final.model, newdata = validation_bin, type = "response")
validation_bin <- validation_bin %>% mutate(INS_hat = ifelse(p_hat > 0.297, 1, 0))
predicted_class = validation_bin$INS_hat
# Compare predicted class with actual values
actual_class <- validation_bin$INS

# Calculate misclassification rate
misclass_rate <- mean(predicted_class != actual_class)
misclass_rate

```

```{r}
library(caret) 
validation_bin$INS = as.factor(validation_bin$INS)
validation_bin$INS_hat = as.factor(validation_bin$INS_hat)
# Create the confusion matrix
confusionMatrix(validation_bin$INS_hat, validation_bin$INS)
```




